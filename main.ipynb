{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, emoji\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "wanted_users = ['BarackObama', 'MichelleObama', 'IvankaTrump', 'DonaldJTrumpJr']\n",
    "for i in range(1,9):\n",
    "    with open(f'tweets/batch_{i}.json') as f:\n",
    "        obj = f.read()\n",
    "        tweets = obj.split('\\n')\n",
    "        for tweet in tweets:\n",
    "            try:\n",
    "                tweet2 = json.loads(tweet)\n",
    "                if tweet2['user']['screen_name'] in wanted_users:\n",
    "                    data.append({\n",
    "                        'created_at': tweet2['created_at'],\n",
    "                        'screen_name': tweet2['user']['screen_name'],\n",
    "                        'full_text': tweet2['full_text']\n",
    "                    })\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7560"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tweets\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count = {\n",
    "    'BarackObama': 0, \n",
    "    'MichelleObama': 0, \n",
    "    'IvankaTrump': 0, \n",
    "    'DonaldJTrumpJr': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    user_count[d['screen_name']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BarackObama': 2788,\n",
       " 'MichelleObama': 1377,\n",
       " 'IvankaTrump': 1712,\n",
       " 'DonaldJTrumpJr': 1683}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tweets by users\n",
    "user_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce number of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count = {\n",
    "    'BarackObama': 0, \n",
    "    'MichelleObama': 0, \n",
    "    'IvankaTrump': 0, \n",
    "    'DonaldJTrumpJr': 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    if user_count[d['screen_name']] < 1300:\n",
    "        data2.append(d)\n",
    "        user_count[d['screen_name']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5200"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data2:\n",
    "    tokens += tweet_tokenizer.tokenize(d['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165257"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160434"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens (url excluded)\n",
    "len([t for t in tokens if t[:4]!='http'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '\\n'.join([str(d) for d in data2])\n",
    "with open('tweets.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclamation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_exclamation = {\n",
    "    'BarackObama': [], \n",
    "    'MichelleObama': [], \n",
    "    'IvankaTrump': [], \n",
    "    'DonaldJTrumpJr': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@BarackObama: 0.07\n",
      "@MichelleObama: 0.36\n",
      "@IvankaTrump: 0.79\n",
      "@DonaldJTrumpJr: 0.62\n"
     ]
    }
   ],
   "source": [
    "for d in data2:\n",
    "    tokens = tweet_tokenizer.tokenize(d['full_text'])\n",
    "    num = sum([1 if t=='!' else 0 for t in tokens])\n",
    "    user_emoji[d['screen_name']].append(num)\n",
    "for user, num in user_emoji.items():\n",
    "    print(f'@{user}: {sum(num)/len(num):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_emoji = {\n",
    "    'BarackObama': [], \n",
    "    'MichelleObama': [], \n",
    "    'IvankaTrump': [], \n",
    "    'DonaldJTrumpJr': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@BarackObama: 0.14\n",
      "@MichelleObama: 0.26\n",
      "@IvankaTrump: 0.42\n",
      "@DonaldJTrumpJr: 0.42\n"
     ]
    }
   ],
   "source": [
    "for d in data2:\n",
    "    tokens = tweet_tokenizer.tokenize(d['full_text'])\n",
    "    num = sum([1 if t in emoji.UNICODE_EMOJI else 0 for t in tokens])\n",
    "    user_exclamation[d['screen_name']].append(num)\n",
    "for user, num in user_exclamation.items():\n",
    "    print(f'@{user}: {sum(num)/len(num):.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:korean-env]",
   "language": "python",
   "name": "conda-env-korean-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
